{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uRLPr0TnIAHO"
   },
   "outputs": [],
   "source": [
    "BRANCH = 'r1.0.0rc1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o_0K1lsW1dj9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nemo_toolkit[nlp] from git+https://github.com/NVIDIA/NeMo.git@#egg=nemo_toolkit[nlp] in /home/ocistudent/anaconda3/lib/python3.8/site-packages (1.0.0rc1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (4.50.2)\n",
      "Requirement already satisfied: torchtext==0.8.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.8.0)\n",
      "Requirement already satisfied: webdataset>=0.1.48 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.1.59)\n",
      "Requirement already satisfied: pangu in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (4.0.6.1)\n",
      "Requirement already satisfied: jieba in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.42.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.23.2)\n",
      "Requirement already satisfied: opencc in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.1.2)\n",
      "Requirement already satisfied: omegaconf>=2.0.5 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (2.0.6)\n",
      "Collecting torchvision==0.8.2\n",
      "  Using cached torchvision-0.8.2-cp38-cp38-manylinux1_x86_64.whl (12.8 MB)\n",
      "Requirement already satisfied: torchaudio==0.7.2 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.7.2)\n",
      "Requirement already satisfied: pytorch-lightning==1.1.5 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.1.5)\n",
      "Requirement already satisfied: transformers>=4.0.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (4.4.2)\n",
      "Requirement already satisfied: torch<=1.7.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (2.8.1)\n",
      "Requirement already satisfied: wrapt in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.19.2)\n",
      "Requirement already satisfied: wget in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (3.2)\n",
      "Requirement already satisfied: hydra-core>=1.0.4 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.0.6)\n",
      "Requirement already satisfied: ruamel.yaml in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.15.87)\n",
      "Requirement already satisfied: onnx>=1.7.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.9.0)\n",
      "Requirement already satisfied: sentencepiece<1.0.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.1.95)\n",
      "Requirement already satisfied: sacremoses>=0.0.43; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (0.0.44)\n",
      "Requirement already satisfied: rapidfuzz; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.4.1)\n",
      "Requirement already satisfied: gdown; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (3.12.2)\n",
      "Requirement already satisfied: sacrebleu[ja]; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.5.1)\n",
      "Requirement already satisfied: unidecode; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.2.0)\n",
      "Requirement already satisfied: youtokentome>=1.0.5; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.0.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.2; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (3.3.2)\n",
      "Requirement already satisfied: h5py; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (2.10.0)\n",
      "Requirement already satisfied: inflect; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (5.3.0)\n",
      "Requirement already satisfied: boto3; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.17.50)\n",
      "Requirement already satisfied: megatron-lm==1.1.5; extra == \"nlp\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from nemo_toolkit[nlp]) (1.1.5)\n",
      "Requirement already satisfied: requests in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from torchtext==0.8.0->nemo_toolkit[nlp]) (2.24.0)\n",
      "Requirement already satisfied: braceexpand in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from webdataset>=0.1.48->nemo_toolkit[nlp]) (0.1.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from scikit-learn->nemo_toolkit[nlp]) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from scikit-learn->nemo_toolkit[nlp]) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from scikit-learn->nemo_toolkit[nlp]) (1.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from omegaconf>=2.0.5->nemo_toolkit[nlp]) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from omegaconf>=2.0.5->nemo_toolkit[nlp]) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from torchvision==0.8.2->nemo_toolkit[nlp]) (8.0.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.18.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (2.5.0)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.8.3)\n",
      "Requirement already satisfied: filelock in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (0.10.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (2020.10.15)\n",
      "Requirement already satisfied: packaging in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (20.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from python-dateutil->nemo_toolkit[nlp]) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from hydra-core>=1.0.4->nemo_toolkit[nlp]) (5.1.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from hydra-core>=1.0.4->nemo_toolkit[nlp]) (4.8)\n",
      "Requirement already satisfied: protobuf in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.15.8)\n",
      "Requirement already satisfied: click in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from sacremoses>=0.0.43; extra == \"nlp\"->nemo_toolkit[nlp]) (7.1.2)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from sacrebleu[ja]; extra == \"nlp\"->nemo_toolkit[nlp]) (2.0.0)\n",
      "Requirement already satisfied: mecab-python3==1.0.3; extra == \"ja\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from sacrebleu[ja]; extra == \"nlp\"->nemo_toolkit[nlp]) (1.0.3)\n",
      "Requirement already satisfied: ipadic<2.0,>=1.0; extra == \"ja\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from sacrebleu[ja]; extra == \"nlp\"->nemo_toolkit[nlp]) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.3.2; extra == \"nlp\"->nemo_toolkit[nlp]) (2.4.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.3.2; extra == \"nlp\"->nemo_toolkit[nlp]) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.3.2; extra == \"nlp\"->nemo_toolkit[nlp]) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.3.2; extra == \"nlp\"->nemo_toolkit[nlp]) (2020.6.20)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.50 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from boto3; extra == \"nlp\"->nemo_toolkit[nlp]) (1.20.50)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from boto3; extra == \"nlp\"->nemo_toolkit[nlp]) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from boto3; extra == \"nlp\"->nemo_toolkit[nlp]) (0.3.6)\n",
      "Requirement already satisfied: pybind11 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from megatron-lm==1.1.5; extra == \"nlp\"->nemo_toolkit[nlp]) (2.6.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.8.0->nemo_toolkit[nlp]) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.8.0->nemo_toolkit[nlp]) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.8.0->nemo_toolkit[nlp]) (3.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.6.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (1.33.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.12.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (50.3.1.post20201107)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.4.4)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (1.30.0)\n",
      "Requirement already satisfied: aiohttp; extra == \"http\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (3.7.4.post0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (4.2.2)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (20.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ocistudent/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.5->nemo_toolkit[nlp]) (0.4.8)\n",
      "Installing collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.1\n",
      "    Uninstalling torchvision-0.9.1:\n",
      "      Successfully uninstalled torchvision-0.9.1\n",
      "Successfully installed torchvision-0.8.2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell\n",
    "\n",
    "# install NeMo\n",
    "#!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dzqD2WDFOIN-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
      "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
     ]
    }
   ],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daYw_Xll2ZR9"
   },
   "source": [
    "# Task Description\n",
    "Given a question and a context both in natural language, predict the span within the context with a start and end position which indicates the answer to the question.\n",
    "For every word in our training dataset we’re going to predict:\n",
    "- likelihood this word is the start of the span \n",
    "- likelihood this word is the end of the span \n",
    "\n",
    "We are using a pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) encoder with 2 span prediction heads for prediction start and end position of the answer. The span predictions are token classifiers consisting of a single linear layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnuziSwJ1yEB"
   },
   "source": [
    "# Dataset\n",
    "This model expects the dataset to be in [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, e.g. a JSON file for each dataset split. \n",
    "In the following we will show example for a training file. Each title has one or multiple paragraph entries, each consisting of the text - \"context\", and question-answer entries. Each question-answer entry has:\n",
    "* a question\n",
    "* a globally unique id\n",
    "* a boolean flag \"is_impossible\" which shows if the question is answerable or not\n",
    "* in case the question is answerable one answer entry, which contains the text span and its starting character index in the context. If not answerable, the \"answers\" list is empty\n",
    "\n",
    "The evaluation files (for validation and testing) follow the above format except for it can provide more than one answer to the same question. \n",
    "The inference file follows the above format except for it does not require the \"answers\" and \"is_impossible\" keywords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXFORGBv2Jqu"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"Super_Bowl_50\", \n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\", \n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"question\": \"Where did Super Bowl 50 take place?\", \n",
    "                            \"is_impossible\": \"false\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ee\", \n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"answer_start\": \"403\", \n",
    "                                    \"text\": \"Santa Clara, California\"\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"question\": \"What was the winning score of the Super Bowl 50?\", \n",
    "                            \"is_impossible\": \"true\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ez\", \n",
    "                            \"answers\": [\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL58EWkd2ZVb"
   },
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrbeXhQ_ZlPH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THi6s1Qx2G1k"
   },
   "source": [
    "In this notebook we are going download the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset to showcase how to do training and inference. There are two datasets, SQuAD1.0 and SQuAD2.0. SQuAD 1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles. SQuAD2.0 dataset combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. \n",
    "\n",
    "\n",
    "To download both datasets, we use  [NeMo/examples/nlp/question_answering/get_squad.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/question_answering/get_squad.py). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tv3qXTTR_hBk"
   },
   "outputs": [],
   "source": [
    "# set the following paths\n",
    "DATA_DIR = \"/home/ocistudent/Desktop/Spring-21/NeMoTutorials\"\n",
    "WORK_DIR = \"/home/ocistudent/Desktop/Spring-21/NeMoTutorials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qcz3Djem_hBn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading get_squad.py...\n"
     ]
    }
   ],
   "source": [
    "## download get_squad.py script to download and preprocess the SQuAD data\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "if not os.path.exists(WORK_DIR + '/get_squad.py'):\n",
    "    print('Downloading get_squad.py...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/get_squad.py', WORK_DIR)\n",
    "else:\n",
    "    print ('get_squad.py already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mpzsC41t_hBq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 12:57:16 get_squad:66] /home/ocistudent/Desktop/Spring-21/NeMo Tutorials\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:49] ** Download file already exists, skipping download\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:49] ** Download file already exists, skipping download\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:49] ** Download file already exists, skipping download\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\r\n",
      "[NeMo I 2021-05-05 12:57:16 get_squad:49] ** Download file already exists, skipping download\r\n"
     ]
    }
   ],
   "source": [
    "# download and preprocess the data\n",
    "! python $WORK_DIR/get_squad.py --destDir $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_HLLl6t_hBs"
   },
   "source": [
    "after execution of the above cell, your data folder will contain a subfolder \"squad\" the following 4 files for training and evaluation\n",
    "- v1.1/train-v1.1.json\n",
    "- v1.1/dev-v1.1.json\n",
    "- v2.0/train-v2.0.json\n",
    "- v2.0/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qYHcfxPL_hBt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/ocistudent/Desktop/Spring-21/NeMo Tutorials/squad':\r\n",
      "v1.1  v2.0\r\n",
      "\r\n",
      "'/home/ocistudent/Desktop/Spring-21/NeMo Tutorials/squad/v1.1':\r\n",
      "dev-v1.1.json  train-v1.1.json\r\n",
      "\r\n",
      "'/home/ocistudent/Desktop/Spring-21/NeMo Tutorials/squad/v2.0':\r\n",
      "dev-v2.0.json  train-v2.0.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls -LR {DATA_DIR}/squad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdpikZVreLlI"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The input into the model is the concatenation of two tokenized sequences:\n",
    "\" [CLS] query [SEP] context [SEP]\".\n",
    "This is the tokenization used for BERT, i.e. [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) Tokenizer, which uses the [Google's BERT vocabulary](https://github.com/google-research/bert). This tokenizer is configured with `model.tokenizer.tokenizer_name=bert-base-uncased` and is automatically instantiated using [Huggingface](https://huggingface.co/)'s API. \n",
    "The benefit of this tokenizer is that this is compatible with a pretrained BERT model, from which we can finetune instead of training the question answering model from scratch. However, we also support other tokenizers, such as `model.tokenizer.tokenizer_name=sentencepiece`. Unlike the BERT WordPiece tokenizer, the [SentencePiece](https://github.com/google/sentencepiece) tokenizer model needs to be first created from a text file.\n",
    "See [02_NLP_Tokenizers.ipynb](https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/nlp/02_NLP_Tokenizers.ipynb) for more details on how to use NeMo Tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q7Y7nyW_hBv"
   },
   "source": [
    "# Data and Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0b0Tn8M_hBv"
   },
   "source": [
    "Note, this is only an example to showcase usage and is not optimized for accuracy. In the following, we will download and adjust the model configuration to create a toy example, where we only use a small fraction of the original dataset. \n",
    "\n",
    "In order to train the full SQuAD model, leave the model parameters from the configuration file unchanged. This sets NUM_SAMPLES=-1 to use the entire dataset, which will slow down performance significantly. We recommend to use bash script and multi-GPU to accelerate this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "n8HZrDmr12_-"
   },
   "outputs": [],
   "source": [
    "# This is the model configuration file that we will download, do not change this\n",
    "MODEL_CONFIG = \"question_answering_squad_config.yaml\"\n",
    "\n",
    "# model parameters, play with these\n",
    "BATCH_SIZE = 12\n",
    "MAX_SEQ_LENGTH = 384\n",
    "# specify BERT-like model, you want to use\n",
    "PRETRAINED_BERT_MODEL = \"bert-base-uncased\"\n",
    "TOKENIZER_NAME = \"bert-base-uncased\" # tokenizer name\n",
    "\n",
    "# Number of data examples used for training, validation, test and inference\n",
    "TRAIN_NUM_SAMPLES = VAL_NUM_SAMPLES = TEST_NUM_SAMPLES = 5000 \n",
    "INFER_NUM_SAMPLES = 5\n",
    "\n",
    "TRAIN_FILE = f\"{DATA_DIR}/squad/v1.1/train-v1.1.json\"\n",
    "VAL_FILE = f\"{DATA_DIR}/squad/v1.1/dev-v1.1.json\"\n",
    "TEST_FILE = f\"{DATA_DIR}/squad/v1.1/dev-v1.1.json\"\n",
    "INFER_FILE = f\"{DATA_DIR}/squad/v1.1/dev-v1.1.json\"\n",
    "\n",
    "INFER_PREDICTION_OUTPUT_FILE = \"output_prediction.json\"\n",
    "INFER_NBEST_OUTPUT_FILE = \"output_nbest.json\"\n",
    "\n",
    "# training parameters\n",
    "LEARNING_RATE = 0.00003\n",
    "\n",
    "# number of epochs\n",
    "MAX_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daludzzL2Jba"
   },
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_whKCxfTMo6Y"
   },
   "source": [
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that will relate to the Model - language model, span prediction, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T1gA8PsJ13MJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file...\n"
     ]
    }
   ],
   "source": [
    "# download the model's default configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/conf/{MODEL_CONFIG}', config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mX3KmWMvSUQw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/Desktop/Spring-21/NeMo\\ Tutorials/configs/question_answering_squad_config.yaml\n",
      "name: QA\n",
      "pretrained_model: null\n",
      "do_training: true\n",
      "trainer:\n",
      "  gpus: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 2\n",
      "  max_steps: null\n",
      "  accumulate_grad_batches: 1\n",
      "  precision: 16\n",
      "  amp_level: O1\n",
      "  accelerator: ddp\n",
      "  gradient_clip_val: 0.0\n",
      "  val_check_interval: 1.0\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  num_sanity_val_steps: 0\n",
      "  log_every_n_steps: 1\n",
      "model:\n",
      "  nemo_path: null\n",
      "  dataset:\n",
      "    version_2_with_negative: false\n",
      "    doc_stride: 128\n",
      "    max_query_length: 64\n",
      "    max_seq_length: 384\n",
      "    max_answer_length: 30\n",
      "    null_score_diff_threshold: 0.0\n",
      "    n_best_size: 20\n",
      "    use_cache: true\n",
      "    do_lower_case: true\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  validation_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  test_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  token_classifier:\n",
      "    num_layers: 1\n",
      "    dropout: 0.0\n",
      "    num_classes: 2\n",
      "    activation: relu\n",
      "    log_softmax: false\n",
      "    use_transformer_init: true\n",
      "  optim:\n",
      "    name: adamw\n",
      "    lr: 3.0e-05\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: SquareRootAnnealing\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.0\n",
      "      last_epoch: -1\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: QA\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire default config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCgWzNBkaQLZ"
   },
   "source": [
    "## Setting up data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds, test_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "Specify data paths using `model.train_ds.file`, `model.valuation_ds.file` and `model.test_ds.file`.\n",
    "\n",
    "Let's now add the data paths to the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LQHCJN-ZaoLp"
   },
   "outputs": [],
   "source": [
    "config.model.train_ds.file = TRAIN_FILE\n",
    "config.model.validation_ds.file = VAL_FILE\n",
    "config.model.test_ds.file = TEST_FILE\n",
    "\n",
    "config.model.train_ds.num_samples = TRAIN_NUM_SAMPLES\n",
    "config.model.validation_ds.num_samples = VAL_NUM_SAMPLES\n",
    "config.model.test_ds.num_samples = TEST_NUM_SAMPLES\n",
    "\n",
    "config.model.tokenizer.tokenizer_name = TOKENIZER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB96-3sTc3yk"
   },
   "source": [
    "# Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
    "\n",
    "Let's first instantiate a Trainer object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knF6QeQQdMrH"
   },
   "outputs": [],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "# For mixed precision training, use precision=16 and amp_level=O1\n",
    "\n",
    "config.trainer.max_epochs = MAX_EPOCHS\n",
    "\n",
    "# Remove distributed training flags if only running on a single GPU or CPU\n",
    "config.trainer.accelerator = None\n",
    "\n",
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IlEMdVxdr6p"
   },
   "source": [
    "# Setting up a NeMo Experiment¶\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uztqGAmdrYt"
   },
   "outputs": [],
   "source": [
    "config.exp_manager.exp_dir = WORK_DIR\n",
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4jy28fbjekD"
   },
   "source": [
    "# Using an Out-Of-Box Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ins2ZzJckKKo"
   },
   "outputs": [],
   "source": [
    "# list available pretrained models\n",
    "nemo_nlp.models.QAModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFnzHvkVk-S5"
   },
   "outputs": [],
   "source": [
    "# load pretained model\n",
    "pretrained_model_name=\"qa_squadv1.1_bertbase\"\n",
    "model = nemo_nlp.models.QAModel.from_pretrained(model_name=pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FI_nQsJo_11"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tjLhUvL_o7_"
   },
   "source": [
    "Before initializing the model, we might want to modify some of the model configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xeuc2i7Y_nP5"
   },
   "outputs": [],
   "source": [
    "# complete list of supported BERT-like models\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RK2xglXyAUOO"
   },
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL\n",
    "config.model.train_ds.batch_size = BATCH_SIZE\n",
    "config.model.validation_ds.batch_size  = BATCH_SIZE\n",
    "config.model.test_ds.batch_size = BATCH_SIZE\n",
    "config.model.optim.lr = LEARNING_RATE\n",
    "\n",
    "print(\"Updated model config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgsGLydWo-6-"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "# dataset we'll be prepared for training and evaluation during\n",
    "model = nemo_nlp.models.QAModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ592Tx4pzyB"
   },
   "source": [
    "## Monitoring Training Progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTJr16_pp0aS"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google import colab\n",
    "  COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "  COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUvnSpyjp0Dh"
   },
   "outputs": [],
   "source": [
    "# start the training\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxBiIKMlH8yv"
   },
   "source": [
    "After training for 1 epoch, exact match on the evaluation data should be around 59.2%, F1 around 70.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynCLBmAWFVsM"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To see how the model performs, let’s run evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBMCoXAKFtSd"
   },
   "outputs": [],
   "source": [
    "model.setup_test_data(test_data_config=config.model.test_ds)\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPdzJVAgSFaJ"
   },
   "source": [
    "# Inference\n",
    "\n",
    "To use the model for creating predictions, let’s run inference on the unlabeled inference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQhsamclRtxJ"
   },
   "outputs": [],
   "source": [
    "# # store test prediction under the experiment output folder\n",
    "output_prediction_file = f\"{exp_dir}/{INFER_PREDICTION_OUTPUT_FILE}\"\n",
    "output_nbest_file = f\"{exp_dir}/{INFER_NBEST_OUTPUT_FILE}\"\n",
    "all_preds, all_nbests = model.inference(file=INFER_FILE, batch_size=5, num_samples=INFER_NUM_SAMPLES, output_nbest_file=output_nbest_file, output_prediction_file=output_prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQpRIOaM_hCQ"
   },
   "outputs": [],
   "source": [
    "for _, item in all_preds.items():\n",
    "    print(f\"question: {item[0]} answer: {item[1]}\")\n",
    "#The prediction file contains the predicted answer to each question id for the first TEST_NUM_SAMPLES.\n",
    "! python -m json.tool $exp_dir/$INFER_PREDICTION_OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ref1qSonGNhP"
   },
   "source": [
    "If you have NeMo installed locally, you can also train the model with \n",
    "[NeMo/examples/nlp/question_answering/get_squad.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/question_answering/question_answering_squad.py).\n",
    "\n",
    "To run training script, use:\n",
    "\n",
    "`python question_answering_squad.py model.train_ds.file=TRAIN_FILE model.validation_ds.file=VAL_FILE model.test_ds.file=TEST_FILE`\n",
    "\n",
    "To improve the performance of the model, train with multi-GPU and a global batch size of 24. So if you use 8 GPUs with `trainer.gpus=8`, set `model.train_ds.batch_size=3`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "daYw_Xll2ZR9"
   ],
   "name": "Question_Answering_Squad.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
