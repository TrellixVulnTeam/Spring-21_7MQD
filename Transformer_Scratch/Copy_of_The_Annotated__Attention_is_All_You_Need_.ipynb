{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wArePITKUgQG"
   },
   "source": [
    "<img src=\"aiayn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSWEk4ttUgQH"
   },
   "source": [
    "> When teaching, I emphasize implementation as a way to understand recent developments in ML. This post is an attempt to keep myself honest along this goal. The recent [\"Attention is All You Need\"]\n",
    "(https://arxiv.org/abs/1706.03762) paper from NIPS 2017 has been instantly impactful paper as a new method for machine translation and potentiall NLP generally. The paper is very clearly written, but the conventional wisdom has been that it is quite difficult to implement correctly. \n",
    ">\n",
    "> In this post I follow the paper through from start to finish and try to implement each component in code. \n",
    "(I have done some minor reordering and skipping from the original paper). This document itself is a working notebook, and should be a completely usable and efficient implementation. To follow along you will first need to install [PyTorch](http://pytorch.org/) and [torchtext](https://github.com/pytorch/text). The complete code is available on [github](https://github.com/harvardnlp/annotated-transformer).\n",
    ">- Alexander \"Sasha\" Rush ([@harvardnlp](https://twitter.com/harvardnlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaYyfFUqUnGY",
    "outputId": "f1f8192a-c5cb-476b-f10c-adf000fb7291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.8.1+cu101)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib spacy torchtext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4LTc4HW7UgQI"
   },
   "outputs": [],
   "source": [
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R749nLNUgQL"
   },
   "source": [
    "* Table of Contents                               \n",
    "{:toc}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esxhOQubUgQL"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M-PiEMOUgQM"
   },
   "source": [
    "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
    "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
    "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
    "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
    "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
    "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
    "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
    "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
    "described in section 3.2.\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
    "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
    "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
    "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned\n",
    "recurrence and have been shown to perform well on simple-language question answering and\n",
    "language modeling tasks [34].\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
    "entirely on self-attention to compute representations of its input and output without using sequencealigned\n",
    "RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
    "self-attention and discuss its advantages over models such as [17, 18] and [9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84vTAA5TUgQM"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-f9BuNsUgQN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBOvyU9BUgQN"
   },
   "source": [
    "Most competitive neural sequence transduction models have an encoder-decoder structure [(cite)](cho2014learning,bahdanau2014neural,sutskever14). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [(cite)](graves2013generating), consuming the previously generated symbols as additional input when generating the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1AC8KeDJUgQO"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base model for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        memory = self.encoder(self.src_embed(src), src_mask)\n",
    "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip0EXqvEUgQQ"
   },
   "source": [
    "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9dznVhQUgQQ"
   },
   "source": [
    "<img src=\"ModalNet-21.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euyRXbaMUgQR"
   },
   "source": [
    "## Encoder and Decoder Stacks   \n",
    "\n",
    "### Encoder: \n",
    "\n",
    "The encoder is composed of a stack of $N=6$ identical layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6yy7pY85UgQR"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "psiq5idJUgQT"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mie9sUeSUgQV"
   },
   "source": [
    "We employ a residual connection [(cite)](he2016deep) around each of the two sub-layers, followed by layer normalization [(cite)](layernorm2016).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sEz9kLClUgQV"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx4On5PCUgQY"
   },
   "source": [
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout [(cite)](srivastava2014dropout) to the output of each sub-layer, before it is added to the sub-layer input and normalized.  \n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zx9JBwAcUgQY"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity we apply the norm first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZduB6mIlUgQa"
   },
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0mEBw9tIUgQb"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHOZnpnBUgQd"
   },
   "source": [
    "### Decoder:\n",
    "\n",
    "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3o_ZB42sUgQd"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOi_W1qaUgQf"
   },
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kMm6xHWVUgQg"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nen9h7wUgQi"
   },
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RyQKI9AgUgQj"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "fgQMtvM-UgQl",
    "outputId": "f9c67ea5-c1fb-4a4d-bbe4-dcadf491b566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6e618a6e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The attention mask shows the position each tgt word (row) is allowed to look at (column).\n",
    "# Words are blocked for attending to future words during training. \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl2E1IaqUgQq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3twSbimFUgQq"
   },
   "source": [
    "### Attention:                                                                                                                                                                                                                                                                               \n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.                                                                                                                                                                                                                                                                                           \n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.                                                                                                         \n",
    "<img width=\"220px\" src=\"ModalNet-19.png\">\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
    "                                                                 \n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$                                                                                                                                                                                                        \n",
    "                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wlZ8zw9PUgQr"
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=0.0):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    # (Dropout described below)\n",
    "    p_attn = F.dropout(p_attn, p=dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV7cIqbrUgQs"
   },
   "source": [
    "The two most commonly used attention functions are additive attention [(cite)](bahdanau2014neural), and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.                                                                                             \n",
    "\n",
    "                                                                        \n",
    "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [(cite)](DBLP:journals/corr/BritzGLL17). We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV7kNMbKUgQt"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiaCxaGGUgQt"
   },
   "source": [
    "### Multi-Head Attention                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.                                                                                                                                                                                                   \n",
    "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n",
    "\n",
    "<img width=\"270px\" src=\"ModalNet-20.png\">\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.                                                                                                                                                                                                                                                                                             \n",
    "    \n",
    "    \n",
    "   \n",
    "$$    \n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.                                                                                                                                                                                                                                                        \n",
    "   \n",
    "\n",
    "   \n",
    "In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_ea0UrEgUgQt"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVdpQ_KwUgQv"
   },
   "source": [
    "### Applications of Attention in our Model                                                                                                                                                      \n",
    "The Transformer uses multi-head attention in three different ways:                                                        \n",
    "1) In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [(cite)](wu2016google, bahdanau2014neural,JonasFaceNet2017).    \n",
    "\n",
    "\n",
    "2) The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   \n",
    "\n",
    "\n",
    "3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.                                                                                                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gERLhK-FUgQw"
   },
   "source": [
    "## Position-wise Feed-Forward Networks                                                                                                                                                                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                        \n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HuDPthO2UgQx"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # Torch linears have a `b` by default. \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68VLwifsUgQz"
   },
   "source": [
    "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [(cite)](press2016using). In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sl5JzPeGUgQz"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_hw5TyCUgQ1"
   },
   "source": [
    "## Positional Encoding                                                                                                                             \n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](JonasFaceNet2017). \n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "$$                                                                                                                                                                                                                                                        \n",
    "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. \n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. \n",
    "                                                                                                                                                                                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MVsjhp6uUgQ1"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "qMsBRCuLUgQ3",
    "outputId": "42cb96c7-37bd-48da-f6cd-cc0b355803bb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The positional encoding will add in a sine wave based on position.\n",
    "# The frequency and offset of the wave is different for each dimension.\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HjtmgH1UgQ5"
   },
   "source": [
    "We also experimented with using learned positional embeddings [(cite)](JonasFaceNet2017) instead, and found that the two versions produced nearly identical results.  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtnFnHH9UgQ6"
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "heaKRIaZUgQ6"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Standard generation step. (Not described in the paper.)\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i97-Y7AUgQ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd3lP9fTUgQ9"
   },
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b-LDhRoaUgQ-"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model object based on hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qP-g4KfhUgQ_",
    "outputId": "e71658a3-e482-4c3e-e421-333c8b848066"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-aa9f90f53c3a>:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "tmp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuV1e8nEUgRB"
   },
   "source": [
    "# Training\n",
    "\n",
    "This section describes the training regime for our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhVxtlZaUgRC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naFpt_GUUgRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X60DPvyaUgRF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz6n3REAUgRH"
   },
   "source": [
    "## Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.  Sentences were encoded using byte-pair encoding \\citep{DBLP:journals/corr/BritzGLL17}, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [(cite)](wu2016google). \n",
    "\n",
    "\n",
    "Sentence pairs were batched together by approximate sequence length.  Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRoJURieUgRH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52xwbbb4UgRK"
   },
   "source": [
    "## Hardware and Schedule                                                                                                                                                                                                   \n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs.  For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.  We trained the base models for a total of 100,000 steps or 12 hours.  For our big models, step time was 1.0 seconds.  The big models were trained for 300,000 steps (3.5 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPp__T_uUgRK"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "We used the Adam optimizer [(cite)](kingma2014adam) with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We varied the learning rate over the course of training, according to the formula:                                                                                            \n",
    "$$                                                                                                                                                                                                                                                                                         \n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
    "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
    "$$                                                                                                                                                                                             \n",
    "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.  We used $warmup\\_steps=4000$.                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Q5yV0f2QUgRL"
   },
   "outputs": [],
   "source": [
    "# Note: This part is incredibly important. \n",
    "# Need to train with this setup of the model is very unstable.\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM5n1PJxUgRN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "PC2wCVEFUgRO",
    "outputId": "1f13c882-85bc-41a6-c9d2-8449fe49222f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tkzxQYKUgRQ"
   },
   "source": [
    "## Regularization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                                                                                                                      \n",
    "### Label Smoothing\n",
    "\n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](DBLP:journals/corr/SzegedyVISW15).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IVWDJLXrUgRQ"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "_gXwt5HVUgRT",
    "outputId": "f483c5ab-ac5a-43b9-c059-efae8562f97d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example\n",
    "crit = LabelSmoothing(5, 0, 0.5)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "sHBHTwmjUgRU",
    "outputId": "304f28de-f963-4c44-a054-11ecdcc3c1d7"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d985f0f7a817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     return crit(Variable(predict.log()),\n\u001b[1;32m     10\u001b[0m                  Variable(torch.LongTensor([1]))).data[0]\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d985f0f7a817>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     return crit(Variable(predict.log()),\n\u001b[1;32m     10\u001b[0m                  Variable(torch.LongTensor([1]))).data[0]\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d985f0f7a817>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                  ])\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     return crit(Variable(predict.log()),\n\u001b[0m\u001b[1;32m     10\u001b[0m                  Variable(torch.LongTensor([1]))).data[0]\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "# Label smoothing starts to penalize the model \n",
    "# if it gets very confident about a given choice\n",
    "crit = LabelSmoothing(5, 0, 0.2)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoyfFgLoUgRW"
   },
   "source": [
    "### Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yVKyONFsUgRW"
   },
   "outputs": [],
   "source": [
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i]) / normalize\n",
    "        total += loss.data[0]\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LiTlbMq2UgRY"
   },
   "outputs": [],
   "source": [
    "def make_std_mask(src, tgt, pad):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sSF9AaKJUgRZ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "                        \n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "q4kFYs7nUgRb"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    model.test()\n",
    "    total = 0\n",
    "    for batch in valid_iter:\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "uJo5AZasUgRd"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_mask = src_mask\n",
    "        self.trg_mask = trg_mask\n",
    "        self.ntokens = ntokens\n",
    "    \n",
    "def data_gen(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
    "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "id": "vrpU5b2sUgRe",
    "outputId": "00871f60-2fcb-4235-c0ab-8de02a0c069c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-aa9f90f53c3a>:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0edc84824e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-8766bbf89844>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_iter, model, criterion, opt, transpose)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-17a408224d43>\u001b[0m in \u001b[0;36mloss_backprop\u001b[0;34m(generator, criterion, out, targets, normalize)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = get_std_opt(model)\n",
    "for epoch in range(2):\n",
    "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S2BcIoOUgRg"
   },
   "source": [
    "# A Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aP_oq0kLUgRh"
   },
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets, legacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKynVliVXg6F",
    "outputId": "e34f2f77-0e65-4fcd-cad7-3d52fdfe3038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.8.1+cu101)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting de_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9MB 17.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (54.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907057 sha256=e8095324edd05fde2767f4a9a51215f3bcd2bdb4416a9014ce4653b3d1e27572\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-olatbk_7/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXtYwdHqUgRj",
    "outputId": "5906874a-a2cc-4e53-f96a-52923cfd16f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-trnmted.tgz: 329MB [00:35, 9.31MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load words from IWSLT\n",
    "\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "print()\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = legacy.data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "TGT = legacy.data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "train, val, test = datasets.IWSLT2017(root='.data', split=('train', 'valid', 'test'), language_pair=('de', 'en'))\n",
    "#train, val, test = datasets.IWSLT2017(exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "#                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "#                                        len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 1\n",
    "SRC.build_vocab(train, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "F8MTIJTWUgRl",
    "outputId": "e260f0e1-0fb6-42ee-c9ee-cf4c4c360ef5"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5deaa21f0644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "# Detail. Batching seems to matter quite a bit. \n",
    "# This is temporary code for dynamic batching based on number of tokens.\n",
    "# This code should all go away once things get merged in this library.\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(legacy.data.iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
    "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
    "\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1900
    },
    "id": "wamR3SPdUgRo",
    "outputId": "b3b92f29-2560-4dbb-c2b0-f2a0b71db37c"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b72405416f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         raise RuntimeError(\n\u001b[1;32m    119\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Create the model an load it onto our GPU.\n",
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SStCCZoiUgRp",
    "outputId": "a551e444-2fe5-4cd5-f46a-3de3505a2545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9.299771845340729 6.987712429686844e-07\n",
      "11 9.415135336574167 4.192627457812107e-06\n",
      "21 8.813630282878876 7.686483672655528e-06\n",
      "31 9.112178653478622 1.118033988749895e-05\n",
      "41 8.607461810112 1.4674196102342371e-05\n",
      "51 8.913826749660075 1.8168052317185794e-05\n",
      "61 8.701497752219439 2.1661908532029216e-05\n",
      "71 8.373274087905884 2.515576474687264e-05\n",
      "81 8.454237446188927 2.8649620961716057e-05\n",
      "91 7.6996782422065735 3.214347717655948e-05\n",
      "101 8.037408232688904 3.56373333914029e-05\n",
      "111 7.704962134361267 3.913118960624633e-05\n",
      "121 7.699015600606799 4.262504582108975e-05\n",
      "131 7.367554426193237 4.611890203593317e-05\n",
      "141 7.2071177661418915 4.961275825077659e-05\n",
      "151 7.106400920893066 5.310661446562001e-05\n",
      "161 6.804656069725752 5.660047068046343e-05\n",
      "171 6.390337720513344 6.0094326895306855e-05\n",
      "181 5.687528342008591 6.358818311015028e-05\n",
      "191 6.122820109128952 6.70820393249937e-05\n",
      "201 5.829070374369621 7.057589553983712e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
    "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NO9lsw2UgRt"
   },
   "source": [
    "\n",
    "OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8BVm-hEUgRw"
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field()\n",
    "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 100\n",
    "train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n",
    "                                    exts=('.en', '.fr'),\n",
    "                                    fields=(SRC, TGT), \n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "SRC.build_vocab(train.src, max_size=50000)\n",
    "TGT.build_vocab(train.trg, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFdZyOIzUgRx",
    "outputId": "bc543dba-c343-47bc-e81e-f74a25688668",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50002, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50004, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=50004)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "print(pad_idx)\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n",
    "model_opt = get_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cinuTkbtUgRz"
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)\n",
    "    valid_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsoeJn4bUgR1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LadFBIEUgR3",
    "outputId": "ba9a812f-b5f6-4972-af91-215d91a223d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50002\n"
     ]
    }
   ],
   "source": [
    "print(pad_idx)\n",
    "print(len(SRC.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP_Au0bHUgR7",
    "outputId": "d5ad5d88-d512-4947-b5b8-f18eaba4f9ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqKKIhoOUgR-",
    "outputId": "2c087978-9803-4639-886b-88df91e62096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.269582842476666 0.0005377044714644026\n",
      "101 3.300532897672383 0.0005726430336128369\n",
      "201 3.3047672072425485 0.0006075815957612711\n",
      "301 2.7151080842595547 0.0006425201579097052\n",
      "401 2.6975380268413574 0.0006774587200581396\n",
      "501 3.051631387323141 0.0007123972822065737\n",
      "601 2.554425454698503 0.000747335844355008\n",
      "701 2.6254820519825444 0.0007822744065034422\n",
      "801 2.868743653933052 0.0008172129686518764\n",
      "901 2.5978208642918617 0.0008521515308003106\n",
      "1001 2.5955790174775757 0.0008870900929487448\n",
      "1101 2.6764775353949517 0.000922028655097179\n",
      "1201 2.464000296778977 0.0009569672172456132\n",
      "1301 2.0503073083236814 0.0009919057793940475\n",
      "1401 2.295472824771423 0.0010268443415424816\n",
      "1501 2.245281406212598 0.0010617829036909158\n",
      "1601 2.2577588511630893 0.00109672146583935\n",
      "1701 2.2232908592559397 0.0011316600279877844\n",
      "1801 2.357596361427568 0.0011665985901362186\n",
      "1901 2.121352154412307 0.0012015371522846527\n",
      "2001 2.5742998471250758 0.001236475714433087\n",
      "2101 2.2518509055953473 0.0012714142765815214\n",
      "2201 2.2251326659170445 0.0013063528387299553\n",
      "2301 2.078994876006618 0.0013412914008783896\n",
      "2401 2.068276036065072 0.001376229963026824\n",
      "2501 2.31435151558253 0.0013907788851585368\n",
      "2601 1.9106871648691595 0.0013738752565588634\n",
      "2701 2.183084836578928 0.0013575733592730722\n",
      "2801 2.4668076275847852 0.0013418383196400342\n",
      "2901 1.963176985620521 0.0013266380295186675\n",
      "3001 2.2140520309330896 0.0013119428705609764\n",
      "3101 2.6989458349489723 0.0012977254713568687\n",
      "3201 2.1293521663174033 0.0012839604929174666\n",
      "3301 2.1402786187827587 0.0012706244386700126\n",
      "3401 2.041781216394156 0.0012576954857216498\n",
      "3501 2.051893091876991 0.0012451533346344698\n",
      "3601 1.5498304846696556 0.001232979075358713\n",
      "3701 2.763939742697403 0.001221155067309524\n",
      "3801 2.7611468499198963 0.0012096648318570434\n",
      "3901 1.7321470333263278 0.0011984929557393293\n",
      "4001 2.139603299088776 0.0011876250041103701\n",
      "4101 2.1966493157087825 0.0011770474421074978\n",
      "4201 2.0962203710805625 0.0011667475639689723\n",
      "4301 1.9717675620922819 0.0011567134288575545\n",
      "4401 2.097687987901736 0.0011469338026529508\n",
      "4501 1.9319786678534001 0.001137398105067946\n",
      "4601 1.8846281475271098 0.0011280963615221983\n",
      "4701 1.9817245414596982 0.0011190191592759865\n",
      "4801 1.7659185670199804 0.0011101576073853326\n",
      "4901 2.188665813198895 0.0011015033000912066\n",
      "5001 2.1391192222399695 0.0010930482833001135\n",
      "5101 1.8125874139368534 0.0010847850238522342\n",
      "5201 1.6616800595074892 0.0010767063813072288\n",
      "5301 1.6544548005331308 0.0010688055820075176\n",
      "5401 1.9542939933016896 0.0010610761952049212\n",
      "5501 2.218412609123334 0.0010535121110594244\n",
      "5601 1.838119359650591 0.001046107520339004\n",
      "5701 1.892627771012485 0.0010388568956672375\n",
      "5801 2.2462481096954434 0.0010317549741811346\n",
      "5901 1.4471426841337234 0.0010247967414755423\n",
      "6001 1.9312338004237972 0.0010179774167228303\n",
      "6101 1.7303275546291843 0.001011292438867507\n",
      "6201 1.8833909621462226 0.0010047374538051973\n",
      "6301 1.8943474531406537 0.0009983083024640838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5940000533591956 0.0009927515780513657\n",
      "101 1.7524283765815198 0.0009865483707369156\n",
      "201 1.900138527940726 0.0009804600111146078\n",
      "301 1.8419977760640904 0.0009744829985071481\n",
      "401 1.9621913449373096 0.0009686139798247046\n",
      "501 2.226916428655386 0.0009628497416600543\n",
      "601 1.7190162097394932 0.0009571872028951208\n",
      "701 1.8589106332874508 0.0009516234077802563\n",
      "801 1.8107321247807704 0.000946155519450957\n",
      "901 1.6531266793608665 0.0009407808138497059\n",
      "1001 1.4840005157748237 0.0009354966740233614\n",
      "1101 1.7578403616789728 0.0009303005847689719\n",
      "1201 1.3920216620899737 0.0009251901276031373\n",
      "1301 1.6626927084289491 0.0009201629760320567\n",
      "1401 1.7256765578058548 0.0009152168911012566\n",
      "1501 1.6049046433763579 0.0009103497172056578\n",
      "1601 1.6955451717367396 0.000905559378142174\n",
      "1701 1.6796367820352316 0.0009008438733884249\n",
      "1801 1.5794002648835885 0.0008962012745924116\n",
      "1901 1.9637197174597532 0.0008916297222591652\n",
      "2001 1.4656428614398465 0.0008871274226214399\n",
      "2101 1.567156056407839 0.0008826926446824871\n",
      "2201 1.542241255287081 0.0008783237174198395\n",
      "2301 1.690121710913445 0.0008740190271398465\n",
      "2401 1.357302049640566 0.0008697770149734477\n",
      "2501 1.9049871656461619 0.0008655961745043597\n",
      "2601 2.240402895025909 0.0008614750495214811\n",
      "2701 1.7940634173137369 0.0008574122318878972\n",
      "2801 1.7314323161263019 0.0008534063595194054\n",
      "2901 1.6064868164248765 0.0008494561144659686\n",
      "3001 1.7515187719254754 0.0008455602210899614\n",
      "3101 1.552100334316492 0.0008417174443354889\n",
      "3201 1.6221882179379463 0.0008379265880834463\n",
      "3301 1.5139061958470847 0.0008341864935873445\n",
      "3401 1.6668659402348567 0.0008304960379852562\n",
      "3501 2.1993618682026863 0.0008268541328835436\n",
      "3601 1.823760490231507 0.0008232597230083089\n",
      "3701 1.8189842144493014 0.0008197117849207771\n",
      "3801 1.689056838164106 0.0008162093257930558\n",
      "3901 1.5656833801185712 0.0008127513822409492\n",
      "4001 1.5621904337021988 0.0008093370192107105\n",
      "4101 1.4836799805052578 0.0008059653289168093\n",
      "4201 1.47899504378438 0.000802635429827976\n",
      "4301 1.6922758186701685 0.0007993464656989501\n",
      "4401 1.636858390578709 0.000796097604645519\n",
      "4501 1.5558803144613194 0.0007928880382605766\n",
      "4601 1.5102424336364493 0.00078971698076907\n",
      "4701 1.541241532890126 0.0007865836682198282\n",
      "4801 1.5931309935403988 0.000783487357712386\n",
      "4901 1.2315586884506047 0.0007804273266570247\n",
      "5001 1.527937745093368 0.0007774028720663579\n",
      "5101 1.31743333209306 0.0007744133098768835\n",
      "5201 1.5960889644484269 0.0007714579742990187\n",
      "5301 1.4181096099782735 0.0007685362171942096\n",
      "5401 1.4596448407392018 0.0007656474074777987\n",
      "5501 1.4594163084111642 0.0007627909305463981\n",
      "5601 1.62109798315214 0.0007599661877285873\n",
      "5701 1.586864550015889 0.0007571725957578231\n",
      "5801 1.5062829439411871 0.0007544095862665088\n",
      "5901 1.4292167258172412 0.0007516766053002225\n",
      "6001 1.4355267270930199 0.0007489731128511653\n",
      "6101 1.4162533966591582 0.0007462985824099354\n",
      "6201 1.6518787188415445 0.0007436525005347853\n",
      "6301 1.5916137372114463 0.0007410343664375577\n",
      "1 1.202994157327339 0.0007387531385993765\n",
      "101 1.4649722938484047 0.0007361862332058686\n",
      "201 1.1459896704182029 0.0007336459004644837\n",
      "301 1.417104929103516 0.0007311316850490442\n",
      "401 1.373963651509257 0.0007286431424819469\n",
      "501 1.6432027550181374 0.0007261798388040814\n",
      "601 1.4122836171882227 0.0007237413502569408\n",
      "701 1.6119428309611976 0.0007213272629763972\n",
      "801 1.5545603609643877 0.0007189371726976359\n",
      "901 1.5427279596333392 0.0007165706844707772\n",
      "1001 1.5437391183004365 0.0007142274123867243\n",
      "1101 1.9743895339342998 0.0007119069793128112\n",
      "1201 1.730805973522365 0.0007096090166378355\n",
      "1301 1.5635135210759472 0.0007073331640260875\n",
      "1401 1.206731209764257 0.000705079069180001\n",
      "1501 1.4495476994197816 0.0007028463876110714\n",
      "1601 1.2935033895773813 0.0007006347824187037\n",
      "1701 1.1734203454107046 0.000698443924076667\n",
      "1801 1.202259551268071 0.0006962734902268488\n",
      "1901 1.7874216835407424 0.0006941231654800159\n",
      "2001 1.5438914835685864 0.0006919926412233024\n",
      "2101 1.5168145569041371 0.000689881615434157\n",
      "2201 1.5306344364071265 0.0006877897925004977\n",
      "2301 1.5227781175635755 0.0006857168830468271\n",
      "2401 1.3308223116910085 0.0006836626037660786\n",
      "2501 1.4871021673316136 0.0006816266772569715\n",
      "2601 1.3415705130901188 0.0006796088318666611\n",
      "2701 1.2746119699440897 0.0006776088015384847\n",
      "2801 1.3439618053671438 0.0006756263256646049\n",
      "2901 1.3065503026737133 0.0006736611489433701\n",
      "3001 1.4918707825127058 0.0006717130212412112\n",
      "3101 1.4003087060991675 0.0006697816974589058\n",
      "3201 1.3473156996478792 0.0006678669374020495\n",
      "3301 1.3869949235959211 0.0006659685056555759\n",
      "3401 1.5086751837225165 0.000664086171462178\n",
      "3501 1.4735991460911464 0.00066221970860449\n",
      "3601 1.3997832712557283 0.0006603688952908887\n",
      "3701 1.5196008981074556 0.0006585335140447885\n",
      "3801 1.2834229312138632 0.0006567133515973014\n",
      "3901 1.3874705795169575 0.0006549081987831418\n",
      "4001 1.6422591609880328 0.0006531178504396635\n",
      "4101 1.305389653716702 0.0006513421053089143\n",
      "4201 1.5159487561322749 0.0006495807659426053\n",
      "4301 1.3981374967552256 0.0006478336386098913\n",
      "4401 1.7390631912276149 0.0006461005332078655\n",
      "4501 1.3604947600979358 0.0006443812631746732\n",
      "4601 1.7799529591429746 0.000642675645405156\n",
      "4701 1.3463407127128448 0.0006409835001689394\n",
      "4801 1.4632963918847963 0.0006393046510308797\n",
      "4901 1.1903231081087142 0.0006376389247737917\n",
      "5001 1.3287691511941375 0.0006359861513233783\n",
      "5101 1.3445309301023372 0.0006343461636752915\n",
      "5201 1.5431754024625661 0.0006327187978242499\n",
      "5301 1.3343850841192761 0.0006311038926951474\n",
      "5401 1.1768817943520844 0.0006295012900760858\n",
      "5501 1.6530805606771537 0.0006279108345532683\n",
      "5601 1.2646167293241888 0.000626332373447694\n",
      "5701 1.3651119051501155 0.000624765756753594\n",
      "5801 1.831987822048177 0.0006232108370785525\n",
      "5901 1.3451470380132378 0.0006216674695852594\n",
      "6001 1.5295006221767835 0.0006201355119348414\n",
      "6101 1.2796215488779126 0.0006186148242317232\n",
      "6201 1.3307579715619795 0.0006171052689699666\n",
      "6301 1.5296110774725094 0.0006156067109810445\n",
      "1 1.355640010209754 0.0006142969713181733\n",
      "101 1.3438594869803637 0.0006128187302418007\n",
      "201 1.3398014856502414 0.0006113511097561582\n",
      "301 1.2453488917089999 0.0006098939832926246\n",
      "401 1.74672898178801 0.0006084472263842588\n",
      "501 1.348103358541266 0.0006070107166211413\n",
      "601 1.2492765338683967 0.0006055843336068713\n",
      "701 1.568915182055207 0.0006041679589161831\n",
      "801 1.3617599749704823 0.0006027614760536461\n",
      "901 1.3296397840604186 0.0006013647704134199\n",
      "1001 1.506301498040557 0.0005999777292400283\n",
      "1101 1.1846984136500396 0.000598600241590126\n",
      "1201 1.1235853107646108 0.0005972321982952243\n",
      "1301 1.3506322290195385 0.0005958734919253515\n",
      "1401 1.5431637589354068 0.0005945240167536175\n",
      "1501 1.4227895765798166 0.0005931836687216574\n",
      "1601 1.2444980588334147 0.0005918523454059284\n",
      "1701 1.37204463215312 0.000590529945984835\n",
      "1801 1.3662666375166737 0.0005892163712066582\n",
      "1901 1.758998476434499 0.0005879115233582672\n",
      "2001 1.3996043455335894 0.0005866153062345879\n",
      "2101 1.409632071852684 0.0005853276251088103\n",
      "2201 1.3139934270293452 0.0005840483867033116\n",
      "2301 1.2863777373568155 0.0005827774991612753\n",
      "2401 1.1966209802776575 0.0005815148720189864\n",
      "2501 1.3174833165830933 0.0005802604161787846\n",
      "2601 1.406668136944063 0.0005790140438826557\n",
      "2701 1.31760111481708 0.0005777756686864456\n",
      "2801 1.22686495014932 0.0005765452054346768\n",
      "2901 1.4871160766715548 0.0005753225702359537\n",
      "3001 1.3321835576352896 0.0005741076804389384\n",
      "3101 1.349290698301047 0.0005729004546088824\n",
      "3201 1.0498975263908505 0.0005717008125046992\n",
      "3301 1.4295434548403136 0.0005705086750565621\n",
      "3401 1.3862976277887356 0.0005693239643440145\n",
      "3501 1.3612052928074263 0.0005681466035745775\n",
      "3601 1.3539716337691061 0.0005669765170628427\n",
      "3701 1.3053378225304186 0.0005658136302100359\n",
      "3801 1.2067344364186283 0.0005646578694840415\n",
      "3901 1.417662046442274 0.0005635091623998715\n",
      "4001 1.2578378450434684 0.0005623674375005725\n",
      "4101 1.2363171717152 0.0005612326243385544\n",
      "4201 1.3426340871083084 0.0005601046534573332\n",
      "4301 1.3097076122212457 0.000558983456373675\n",
      "4401 1.0131576862186193 0.0005578689655601316\n",
      "4501 1.4332989812392043 0.000556761114427959\n",
      "4601 1.4043821960221976 0.0005556598373104054\n",
      "4701 1.373746110650245 0.0005545650694463629\n",
      "4801 1.2657524709356949 0.0005534767469643717\n",
      "4901 1.1224889098666608 0.0005523948068669684\n",
      "5001 1.2615516305086203 0.000551319187015369\n",
      "5101 1.409785834257491 0.0005502498261144795\n",
      "5201 1.3791224808810512 0.0005491866636982242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301 1.2408291140163783 0.0005481296401151859\n",
      "5401 1.3008261130889878 0.0005470786965145471\n",
      "5501 1.1700160388209042 0.0005460337748323287\n",
      "5601 1.2999350049067289 0.000544994817777915\n",
      "5701 1.3322585223941132 0.0005439617688208604\n",
      "5801 1.254337038320955 0.0005429345721779703\n",
      "5901 1.773689029644629 0.000541913172800649\n",
      "6001 1.3898115772462916 0.0005408975163625087\n",
      "6101 1.4735579792177305 0.0005398875492472326\n",
      "6201 1.05738219874911 0.000538883218536687\n",
      "6301 1.0802461032290012 0.0005378844719992749\n",
      "1 1.286231731530279 0.0005370101533168812\n",
      "101 1.2250633136718534 0.0005360217659787991\n",
      "201 1.239320948603563 0.0005350388161199592\n",
      "301 1.4140636462761904 0.0005340612540665886\n",
      "401 1.442663955502212 0.0005330890307779102\n",
      "501 1.4505203103472013 0.0005321220978358095\n",
      "601 1.2115196966333315 0.0005311604074347066\n",
      "701 1.2035035027656704 0.0005302039123716286\n",
      "801 1.3747974793659523 0.0005292525660364788\n",
      "901 1.36490419106849 0.0005283063224024965\n",
      "1001 1.1864821948111057 0.0005273651360169036\n",
      "1101 1.1623371304303873 0.000526428961991735\n",
      "1201 1.1043747729854658 0.0005254977559948457\n",
      "1301 1.6982813560443901 0.0005245714742410941\n",
      "1401 1.2719842366641387 0.0005236500734836944\n",
      "1501 1.2951120301149786 0.0005227335110057353\n",
      "1601 1.580276207998395 0.0005218217446118628\n",
      "1701 1.218743062199792 0.0005209147326201215\n",
      "1801 1.1479590674862266 0.0005200124338539494\n",
      "1901 1.2872504810075043 0.0005191148076343284\n",
      "2001 1.8993003838438653 0.0005182218137720798\n",
      "2101 1.2762204335303977 0.0005173334125603075\n",
      "2201 1.6183682525045242 0.0005164495647669814\n",
      "2301 1.2522982619411778 0.0005155702316276618\n",
      "2401 1.2925108795752749 0.0005146953748383575\n",
      "2501 1.340747339767404 0.0005138249565485178\n",
      "2601 1.340512964350637 0.0005129589393541545\n",
      "2701 1.1672844442073256 0.0005120972862910908\n",
      "2801 1.257948145037517 0.0005112399608283344\n",
      "2901 1.510728154462413 0.0005103869268615725\n",
      "3001 1.4130934766726568 0.0005095381487067851\n",
      "3101 1.2367545471934136 0.0005086935910939762\n",
      "3201 1.3846962348325178 0.0005078532191610173\n",
      "3301 1.2582954101526411 0.0005070169984476032\n",
      "3401 1.1545094328466803 0.0005061848948893172\n",
      "3501 1.295005505089648 0.0005053568748118022\n",
      "3601 1.3319955187034793 0.0005045329049250373\n",
      "3701 1.3548947679810226 0.000503712952317716\n",
      "3801 1.4635376840888057 0.0005028969844517252\n",
      "3901 1.6542128916307774 0.0005020849691567213\n",
      "4001 1.3512894048908493 0.0005012768746248036\n",
      "4101 1.397591198408918 0.0005004726694052806\n",
      "4201 1.3055676214280538 0.0004996723223995292\n",
      "4301 1.3375271083787084 0.000498875802855943\n",
      "4401 1.2366086341207847 0.0004980830803649704\n",
      "4501 1.2439679206581786 0.0004972941248542376\n",
      "4601 1.352382222772576 0.0004965089065837576\n",
      "4701 1.7570512742054234 0.0004957273961412208\n",
      "4801 1.232903058291413 0.0004949495644373684\n",
      "4901 1.015858386293985 0.0004941753827014446\n",
      "5001 1.381107110035373 0.000493404822476726\n",
      "5101 0.9564947709441185 0.0004926378556161293\n",
      "5201 1.228621664486127 0.0004918744542778926\n",
      "5301 1.182083563413471 0.0004911145909213302\n",
      "5401 1.2583643229590962 0.0004903582383026592\n",
      "5501 1.404046923678834 0.0004896053694708976\n",
      "5601 1.2389367091745953 0.0004888559577638302\n",
      "5701 1.119320425321348 0.00048810997680404295\n",
      "5801 1.586507015679672 0.0004873674004950231\n",
      "5901 1.112720330056618 0.0004866282030173253\n",
      "6001 1.3577893248293549 0.0004858923588248005\n",
      "6101 1.217524498468265 0.0004851598426408882\n",
      "6201 1.3229771983387764 0.0004844306294549693\n",
      "6301 1.5693217546272535 0.0004837046945187796\n",
      "1 1.1786362157727126 0.00048306134975017534\n",
      "101 1.28241519164294 0.00048234154403106603\n",
      "201 1.1411214591062162 0.00048162494648183897\n",
      "301 1.2352831599419005 0.0004809115333417623\n",
      "401 1.1032181181944907 0.00048020128109574806\n",
      "501 1.18390864826506 0.00047949416647109663\n",
      "601 1.2226583541632863 0.00047879016643429347\n",
      "701 1.0373018080717884 0.0004780892581878584\n",
      "801 1.2819566036341712 0.00047739141916724456\n",
      "901 1.1648676298791543 0.0004766966270377871\n",
      "1001 1.1654199322802015 0.00047600485969170105\n",
      "1101 1.2386636545270449 0.00047531609524512704\n",
      "1201 1.2253044219105504 0.0004746303120352227\n",
      "1301 1.375744077755371 0.0004739474886173019\n",
      "1401 1.1551300736318808 0.0004732676037620178\n",
      "1501 1.5255512128351256 0.00047259063645259034\n",
      "1601 1.255034319277911 0.00047191656588207824\n",
      "1701 1.1623500876303297 0.0004712453714506923\n",
      "1801 1.2958592986833537 0.00047057703276315175\n",
      "1901 1.1341320046922192 0.0004699115296260807\n",
      "2001 1.1937441515619867 0.0004692488420454462\n",
      "2101 1.7062073841661913 0.00046858895022403485\n",
      "2201 1.2566360468044877 0.00046793183455896863\n",
      "2301 1.2216275975806639 0.0004672774756392595\n",
      "2401 1.2636524712725077 0.0004666258542434008\n",
      "2501 1.2113699619076215 0.00046597695133699556\n",
      "2601 1.1559934263350442 0.00046533074807042176\n",
      "2701 1.256740387296304 0.0004646872257765319\n",
      "2801 1.3039579528664262 0.0004640463659683885\n",
      "2901 1.2651300196012016 0.0004634081503370334\n",
      "3001 1.2652980692801066 0.000462772560749291\n",
      "3101 1.1218284339411184 0.00046213957924560355\n",
      "3201 1.2543016897689085 0.0004615091880379007\n",
      "3301 1.2131407480192138 0.0004608813695074994\n",
      "3401 1.2994702684518415 0.0004602561062030357\n",
      "3501 1.2115506358095445 0.00045963338083842724\n",
      "3601 1.1760960748360958 0.00045901317629086643\n",
      "3701 1.0682971130590886 0.00045839547559884254\n",
      "3801 1.0764332090620883 0.00045778026196019347\n",
      "3901 1.1835216325707734 0.0004571675187301866\n",
      "4001 1.3529939632862806 0.00045655722941962654\n",
      "4101 1.3684578015236184 0.0004559493776929923\n",
      "4201 1.2233722301607486 0.0004553439473666001\n",
      "4301 1.2596116681525018 0.0004547409224067939\n",
      "4401 1.2757911044172943 0.00045414028692816196\n",
      "4501 1.2199301174841821 0.0004535420251917793\n",
      "4601 1.3471774608151463 0.0004529461216034753\n",
      "4701 1.475795219448628 0.0004523525607121267\n",
      "4801 1.1835241899825633 0.0004517613272079745\n",
      "4901 1.1791616377497576 0.0004511724059209659\n",
      "5001 1.3126113665202865 0.0004505857818191191\n",
      "5101 1.2516068609402282 0.0004500014400069121\n",
      "5201 1.178165558274486 0.0004494193657236937\n",
      "5301 1.6013869942435122 0.0004488395443421177\n",
      "5401 1.2677101592962572 0.00044826196136659916\n",
      "5501 1.1976667390699731 0.0004476866024317922\n",
      "5601 1.1990807302790927 0.00044711345330108884\n",
      "5701 1.1415361673789448 0.0004465424998651406\n",
      "5801 1.2389779405202717 0.0004459737281403985\n",
      "5901 1.1746156329172663 0.0004454071242676752\n",
      "6001 1.1718775559565984 0.0004448426745107265\n",
      "6101 1.1669323876558337 0.00044428036525485275\n",
      "6201 1.22836275130976 0.0004437201830055194\n",
      "6301 1.1068585112225264 0.000443162114386997\n",
      "1 1.1908240653865505 0.00044267275186678196\n",
      "101 1.156728027795907 0.0004421186210736662\n",
      "201 1.151486962888157 0.0004415665660409348\n",
      "301 1.1075408830074593 0.0004410165738412884\n",
      "401 1.1251853418070823 0.00044046863165985925\n",
      "501 1.224421168473782 0.0004399227267929559\n",
      "601 1.1097798637929372 0.00043937884664682695\n",
      "701 0.992531725903973 0.0004388369787364407\n",
      "801 1.2762772621936165 0.0004382971106842813\n",
      "901 1.154728337773122 0.00043775923021916087\n",
      "1001 0.9699444866273552 0.00043722332517504866\n",
      "1101 1.1039727496681735 0.0004366893834899152\n",
      "1201 1.2997219555545598 0.0004361573932045913\n",
      "1301 1.5713044246076606 0.00043562734246164385\n",
      "1401 1.1782071397465188 0.00043509921950426545\n",
      "1501 1.256332863289117 0.0004345730126751789\n",
      "1601 1.162631830346072 0.00043404871041555687\n",
      "1701 1.1123517343075946 0.00043352630126395546\n",
      "1801 1.0946980192093179 0.0004330057738552615\n",
      "1901 1.120711475959979 0.0004324871169196544\n",
      "2001 1.1385652619646862 0.0004319703192815812\n",
      "2101 1.0391206528292969 0.0004314553698587452\n",
      "2201 1.1468603002722375 0.00043094225766110786\n",
      "2301 1.1944863148819422 0.0004304309717899036\n",
      "2401 1.1445480604888871 0.00042992150143666746\n",
      "2501 1.160092411795631 0.0004294138358822756\n",
      "2601 0.905779943568632 0.00042890796449599795\n",
      "2701 1.2337692737637553 0.0004284038767345632\n",
      "2801 1.2654334787439439 0.00042790156214123586\n",
      "2901 1.2613030684588011 0.0004274010103449054\n",
      "3001 1.1566388571663992 0.0004269022110591865\n",
      "3101 1.1506170178181492 0.0004264051540815317\n",
      "3201 1.1042177192866802 0.00042590982929235444\n",
      "3301 1.268968387885252 0.00042541622665416415\n",
      "3401 1.1708880871301517 0.0004249243362107117\n",
      "3501 1.1094016103306785 0.00042443414808614573\n",
      "3601 1.3188527839665767 0.0004239456524841804\n",
      "3701 1.2144307589042 0.0004234588396872726\n",
      "3801 1.1827894128946355 0.0004229737000558104\n",
      "3901 0.9924444004427642 0.00042249022402731095\n",
      "4001 1.1228576390712988 0.0004220084021156294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101 1.1924936635477934 0.0004215282249101765\n",
      "4201 1.1275967326655518 0.0004210496830751471\n",
      "4301 1.0625419117277488 0.0004205727673487576\n",
      "4401 1.1389823842037003 0.00042009746854249313\n",
      "4501 1.339291847194545 0.00041962377754036395\n",
      "4601 1.0302886090357788 0.0004191516852981713\n",
      "4701 1.7122778899138211 0.00041868118284278167\n",
      "4801 1.2910672437865287 0.0004182122612714111\n",
      "4901 1.0494152382598259 0.00041774491175091685\n",
      "5001 1.1782474033534527 0.0004172791255170995\n",
      "5101 1.040663594380021 0.0004168148938740118\n",
      "5201 0.9901199785526842 0.00041635220819327733\n",
      "5301 1.5490817801910453 0.00041589105991341656\n",
      "5401 1.1346296942792833 0.00041543144053918197\n",
      "5501 1.223581779631786 0.00041497334164089994\n",
      "5601 0.958975835936144 0.0004145167548538224\n",
      "5701 1.238148811913561 0.0004140616718774844\n",
      "5801 1.1881117207813077 0.000413608084475071\n",
      "5901 1.1295715225860476 0.0004131559844727907\n",
      "6001 1.0610488005331717 0.000412705363759257\n",
      "6101 1.2371235750615597 0.00041225621428487707\n",
      "6201 1.1479767516138963 0.00041180852806124783\n",
      "6301 1.2059640220250003 0.0004113622971605593\n",
      "1 1.1765662879188312 0.0004109663692823915\n",
      "101 1.219779463717714 0.0004105228675028437\n",
      "201 0.972488499362953 0.00041008079847008953\n",
      "301 1.1617506150214467 0.0004096401544864815\n",
      "401 1.0883347367926035 0.0004092009279121472\n",
      "501 1.1085488446406089 0.00040876311116443343\n",
      "601 1.1023443718672752 0.0004083266967173559\n",
      "701 1.018611608800711 0.00040789167710105623\n",
      "801 1.1658196483003849 0.00040745804490126497\n",
      "901 1.2917855954219704 0.0004070257927587706\n",
      "1001 1.186474629881559 0.00040659491336889525\n",
      "1101 1.127356821874855 0.00040616539948097586\n",
      "1201 1.1975307842949405 0.00040573724389785204\n",
      "1301 1.1174790017685154 0.0004053104394753595\n",
      "1401 1.0863252188792103 0.0004048849791218294\n",
      "1501 1.0622602235816885 0.000404460855797593\n",
      "1601 1.1195327076129615 0.00040403806251449327\n",
      "1701 1.2447982146404684 0.00040361659233540054\n",
      "1801 1.2607627244724426 0.0004031964383737348\n",
      "1901 1.278331945562968 0.00040277759379299307\n",
      "2001 1.025594950420782 0.0004023600518062819\n",
      "2101 1.115274733179831 0.0004019438056758561\n",
      "2201 1.1167924739420414 0.0004015288487126612\n",
      "2301 1.0995151306560729 0.000401115174275883\n",
      "2401 1.1547567544039339 0.00040070277577250023\n",
      "2501 1.1590442548913416 0.00040029164665684384\n",
      "2601 1.1047132272506133 0.00039988178043016053\n",
      "2701 1.0620037270709872 0.00039947317064018093\n",
      "2801 1.0939110746548977 0.00039906581088069363\n",
      "2901 0.9693534299731255 0.0003986596947911227\n",
      "3001 1.2754340882529505 0.0003982548160561108\n",
      "3101 2.0011723663365046 0.0003978511684051071\n",
      "3201 1.1672507325711194 0.0003974487456119586\n",
      "3301 0.8547125565819442 0.00039704754149450736\n",
      "3401 1.0948779656609986 0.00039664754991419163\n",
      "3501 1.1662541554399013 0.0003962487647756509\n",
      "3601 1.242357063729287 0.00039585118002633614\n",
      "3701 1.142975198366912 0.000395454789656124\n",
      "3801 1.2055247909738682 0.0003950595876969351\n",
      "3901 1.2129587295930833 0.0003946655682223565\n",
      "4001 1.239052205113694 0.0003942727253472687\n",
      "4101 1.1285374546132516 0.0003938810532274764\n",
      "4201 1.3123125492420513 0.00039349054605934306\n",
      "4301 1.2088057449145708 0.00039310119807943006\n",
      "4401 1.0897887839237228 0.00039271300356413926\n",
      "4501 1.2131327866227366 0.00039232595682935973\n",
      "4601 0.9877158605959266 0.000391940052230118\n",
      "4701 1.2145014504967548 0.0003915552841602323\n",
      "4801 1.1513765653944574 0.0003911716470519708\n",
      "4901 1.1751896993955597 0.0003907891353757127\n",
      "5001 1.1771067604749987 0.0003904077436396139\n",
      "5101 1.3224336538114585 0.0003900274663892758\n",
      "5201 1.355893952131737 0.0003896482982074174\n",
      "5301 1.1996791153214872 0.0003892702337135512\n",
      "5401 1.206806231304654 0.0003888932675636631\n",
      "5501 1.280991047504358 0.0003885173944498942\n",
      "5601 0.9168080711970106 0.0003881426091002278\n",
      "5701 1.1924245768459514 0.0003877689062781782\n",
      "5801 1.1940782586170826 0.0003873962807824836\n",
      "5901 1.2784329915011767 0.0003870247274468023\n",
      "6001 1.1448089724872261 0.00038665424113941134\n",
      "6101 1.1181278676813236 0.0003862848167629092\n",
      "6201 1.2686003018752672 0.00038591644925392126\n",
      "6301 1.0795898175565526 0.000385549133582808\n",
      "1 1.199639980099164 0.00038523407955521927\n",
      "101 1.0967486363369972 0.00038486870703897236\n",
      "201 1.2039306252845563 0.00038450437215947677\n",
      "301 1.106695241353009 0.0003841410700146326\n",
      "401 1.0133273621067929 0.00038377879573470126\n",
      "501 1.1209047999582253 0.0003834175444820315\n",
      "601 1.073817516444251 0.00038305731145078797\n",
      "701 1.1002086726948619 0.00038269809186668256\n",
      "801 1.044247637852095 0.00038233988098670897\n",
      "901 1.1538543488713913 0.00038198267409887953\n",
      "1001 1.1638364950194955 0.00038162646652196454\n",
      "1101 1.1222136826545466 0.00038127125360523515\n",
      "1201 1.0936923912668135 0.0003809170307282081\n",
      "1301 1.0790816302178428 0.0003805637933003932\n",
      "1401 1.0973517978854943 0.0003802115367610436\n",
      "1501 1.3543376340385294 0.00037986025657890806\n",
      "1601 1.0638599513913505 0.0003795099482519871\n",
      "1701 1.095864930888638 0.0003791606073072896\n",
      "1801 1.3785420355270617 0.00037881222930059356\n",
      "1901 1.0656100183841772 0.0003784648098162084\n",
      "2001 1.083574770949781 0.0003781183444667399\n",
      "2101 1.7192173111798184 0.0003777728288928577\n",
      "2201 1.090653446619399 0.0003774282587630644\n",
      "2301 1.1122783308528597 0.00037708462977346826\n",
      "2401 0.95696423901245 0.0003767419376475568\n",
      "2501 1.2160079335735645 0.0003764001781359734\n",
      "2601 1.2086039673304185 0.00037605934701629616\n",
      "2701 1.0832101813866757 0.00037571944009281874\n",
      "2801 1.013074157119263 0.00037538045319633314\n",
      "2901 1.0823434699559584 0.00037504238218391556\n",
      "3001 1.1612105248786975 0.0003747052229387128\n",
      "3101 0.9126896761590615 0.0003743689713697328\n",
      "3201 1.3341417479550728 0.00037403362341163505\n",
      "3301 0.9600269035436213 0.0003736991750245252\n",
      "3401 1.1916928359714802 0.0003733656221937497\n",
      "3501 1.4976106537505984 0.0003730329609296942\n",
      "3601 1.1840611910447478 0.0003727011872675824\n",
      "3701 1.3150727476167958 0.0003723702972672783\n",
      "3801 1.221188339870423 0.00037204028701308904\n",
      "3901 1.2026138188084587 0.0003717111526135708\n",
      "4001 1.3793403076779214 0.00037138289020133557\n",
      "4101 1.0772470782976598 0.0003710554959328607\n",
      "4201 1.0168679640773917 0.0003707289659882998\n",
      "4301 1.1685322925950459 0.00037040329657129513\n",
      "4401 1.1224966624286026 0.00037007848390879306\n",
      "4501 1.0253048577578738 0.00036975452425085955\n",
      "4601 1.2101853350850433 0.00036943141387049916\n",
      "4701 1.050108958443161 0.0003691091490634741\n",
      "4801 1.2717001468117815 0.00036878772614812674\n",
      "4901 1.1231291381409392 0.00036846714146520227\n",
      "5001 1.1141781120968517 0.00036814739137767423\n",
      "5101 0.9994667179416865 0.00036782847227057074\n",
      "5201 1.3557415267750912 0.0003675103805508032\n",
      "5301 1.504937146051816 0.0003671931126469962\n",
      "5401 1.0444834220979828 0.00036687666500931896\n",
      "5501 1.0159150707913795 0.0003665610341093186\n",
      "5601 1.4691102042561397 0.00036624621643975515\n",
      "5701 1.2679062836105004 0.0003659322085144373\n",
      "5801 1.1070539963402553 0.0003656190068680607\n",
      "5901 1.2043958652066067 0.0003653066080560474\n",
      "6001 1.1217296464601532 0.00036499500865438625\n",
      "6101 1.2132740695233224 0.00036468420525947586\n",
      "6201 1.452793362134571 0.00036437419448796804\n",
      "6301 1.0731251265387982 0.00036406497297661317\n",
      "1 1.1998733360724145 0.00036379350826718935\n",
      "101 1.1498671763110906 0.0003634857615296514\n",
      "201 1.1022596344992053 0.00036317879447701637\n",
      "301 1.0011312331771478 0.00036287260382257964\n",
      "401 1.0373523531015962 0.0003625671862990008\n",
      "501 1.0644397677097004 0.000362262538658157\n",
      "601 1.1183270415349398 0.000361958657670998\n",
      "701 0.9439565273642074 0.00036165554012740277\n",
      "801 1.0483181119780056 0.0003613531828360362\n",
      "901 1.10791165966657 0.00036105158262420917\n",
      "1001 0.9895736404578201 0.00036075073633773743\n",
      "1101 1.0942751504917396 0.00036045064084080426\n",
      "1201 1.0274720180314034 0.0003601512930158222\n",
      "1301 1.049829078532639 0.0003598526897632977\n",
      "1401 1.0599873741302872 0.00035955482800169595\n",
      "1501 1.1110646773013286 0.00035925770466730756\n",
      "1601 1.1195740707335062 0.0003589613167141159\n",
      "1701 1.1175601889844984 0.0003586656611136664\n",
      "1801 1.27650167158572 0.00035837073485493607\n",
      "1901 1.1153064398095012 0.000358076534944205\n",
      "2001 1.4756392514391337 0.000357783058404929\n",
      "2101 0.8967400579713285 0.0003574903022776121\n",
      "2201 1.0554919667192735 0.0003571982636196827\n",
      "2301 1.1484477042686194 0.000356906939505368\n",
      "2401 1.0967925000586547 0.00035661632702557175\n",
      "2501 1.275310180048109 0.0003563264232877516\n",
      "2601 1.084061863599345 0.00035603722541579873\n",
      "2701 1.076280384673737 0.00035574873054991784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801 1.200010517553892 0.0003554609358465082\n",
      "2901 1.059172057226533 0.000355173838478046\n",
      "3001 1.0261963941156864 0.000354887435632968\n",
      "3101 0.9737269952893257 0.0003546017245155551\n",
      "3201 1.109491402952699 0.0003543167023458187\n",
      "3301 1.0379895093501545 0.0003540323663593864\n",
      "3401 1.0210047286818735 0.00035374871380738974\n",
      "3501 1.3062616163679195 0.0003534657419563522\n",
      "3601 1.1297821700572968 0.00035318344808807914\n",
      "3701 1.095454223890556 0.0003529018294995477\n",
      "3801 1.0263875699602067 0.00035262088350279793\n",
      "3901 1.0200049103004858 0.00035234060742482575\n",
      "4001 1.139240143907955 0.00035206099860747537\n",
      "4101 1.185085133272878 0.00035178205440733397\n",
      "4201 1.0887831579057092 0.0003515037721956263\n",
      "4301 1.7533796445081862 0.000351226149358111\n",
      "4401 1.1149956742010545 0.00035094918329497705\n",
      "4501 1.1544227619015146 0.0003506728714207421\n",
      "4601 1.1121961249154992 0.00035039721116415036\n",
      "4701 1.0929120010696352 0.0003501221999680728\n",
      "4801 1.118996370700188 0.000349847835289407\n",
      "4901 0.9860638748505153 0.00034957411459897886\n",
      "5001 1.004449057742022 0.0003493010353814441\n",
      "5101 1.2927988782757893 0.00034902859513519165\n",
      "5201 0.98420900356723 0.0003487567913722472\n",
      "5301 1.1210692654858576 0.000348485621618178\n",
      "5401 1.163566045666812 0.00034821508341199766\n",
      "5501 1.17701395630138 0.0003479451743060731\n",
      "5601 1.0291424575298151 0.00034767589186603104\n",
      "5701 1.2543358486509533 0.0003474072336706659\n",
      "5801 1.2299754508348997 0.00034713919731184855\n",
      "5901 1.1936145080917413 0.0003468717803944353\n",
      "6001 0.9138605990447104 0.00034660498053617827\n",
      "6101 1.1037570714397589 0.00034633879536763624\n",
      "6201 1.04157008238235 0.00034607322253208626\n",
      "6301 1.1919174637878314 0.0003458082596854357\n",
      "1 0.9797578346915543 0.00034557559511139293\n",
      "101 0.9891712895187084 0.00034531177274179953\n",
      "201 1.1815550197497942 0.00034504855367990236\n",
      "301 1.1358435613219626 0.0003447859356297997\n",
      "401 1.0717519058380276 0.000344523916307803\n",
      "501 1.172375235328218 0.00034426249344235384\n",
      "601 1.0603420100815129 0.00034400166477394084\n",
      "701 1.0992282917286502 0.000343741428055018\n",
      "801 1.04559408465866 0.0003434817810499231\n",
      "901 1.0248865495998416 0.0003432227215347973\n",
      "1001 1.0598302248690743 0.0003429642472975047\n",
      "1101 1.0938185814011376 0.0003427063561375535\n",
      "1201 1.291374852447916 0.000342449045866017\n",
      "1301 1.0285806620959193 0.0003421923143054557\n",
      "1401 1.17992360109929 0.0003419361592898398\n",
      "1501 0.9615428688703105 0.0003416805786644727\n",
      "1601 1.2486475716141285 0.0003414255702859144\n",
      "1701 0.9794061238644645 0.0003411711320219065\n",
      "1801 1.1059001302346587 0.00034091726175129706\n",
      "1901 0.9131126408465207 0.00034066395736396637\n",
      "2001 0.9930663524428383 0.0003404112167607534\n",
      "2101 1.0812218267819844 0.0003401590378533823\n",
      "2201 1.0715046060213353 0.0003399074185643907\n",
      "2301 1.1185214965953492 0.00033965635682705713\n",
      "2401 1.05721441534115 0.00033940585058533\n",
      "2501 1.0936700437378022 0.00033915589779375693\n",
      "2601 1.07034515045234 0.00033890649641741454\n",
      "2701 1.0813248989579733 0.00033865764443183875\n",
      "2801 1.0510470166627783 0.0003384093398229561\n",
      "2901 0.9623011860530823 0.0003381615805870148\n",
      "3001 1.1494725269731134 0.00033791436473051725\n",
      "3101 1.2335324875239166 0.0003376676902701525\n",
      "3201 1.0398004396120086 0.00033742155523272933\n",
      "3301 1.0589452146668918 0.0003371759576551101\n",
      "3401 1.084106142167002 0.00033693089558414497\n",
      "3501 1.0406659920408856 0.0003366863670766065\n",
      "3601 0.9325393754988909 0.00033644237019912526\n",
      "3701 1.0507557194505353 0.0003361989030281253\n",
      "3801 0.945562198292464 0.0003359559636497606\n",
      "3901 1.0489263081690297 0.0003357135501598519\n",
      "4001 1.0528855019947514 0.00033547166066382383\n",
      "4101 1.1952165458351374 0.0003352302932766432\n",
      "4201 1.0958911241032183 0.00033498944612275674\n",
      "4301 1.077680416405201 0.0003347491173360301\n",
      "4401 1.2972540742484853 0.0003345093050596873\n",
      "4501 1.039087069220841 0.0003342700074462501\n",
      "4601 0.9597453814931214 0.00033403122265747876\n",
      "4701 1.1728105103829876 0.00033379294886431207\n",
      "4801 1.1258336059836438 0.0003335551842468092\n",
      "4901 1.0011352995352354 0.0003333179269940906\n",
      "5001 0.9672558718448272 0.00033308117530428074\n",
      "5101 1.0522874586749822 0.0003328449273844502\n",
      "5201 1.0063609674107283 0.0003326091814505589\n",
      "5301 1.0480196221014921 0.00033237393572739917\n",
      "5401 1.0445173801199417 0.00033213918844854004\n",
      "5501 1.417743748796056 0.00033190493785627127\n",
      "5601 1.0902981872641249 0.000331671182201548\n",
      "5701 1.0301871165866032 0.00033143791974393625\n",
      "5801 1.4090074983541854 0.00033120514875155805\n",
      "5901 1.1904211936052889 0.0003309728675010378\n",
      "6001 1.1052953382313717 0.0003307410742774485\n",
      "6101 1.133972127106972 0.00033050976737425853\n",
      "6201 1.4820798086614104 0.00033027894509327907\n",
      "6301 1.1476092239608988 0.00033004860574461153\n",
      "1 1.0870586273958907 0.00032984630526377065\n",
      "101 1.00110832543578 0.00032961686928176145\n",
      "201 0.9876259700540686 0.0003293879114110055\n",
      "301 1.2413200094233616 0.0003291594299932822\n",
      "401 0.9424758276436478 0.00032893142337841173\n",
      "501 1.0926933737646323 0.00032870388992420444\n",
      "601 1.0706572374765528 0.0003284768279964114\n",
      "701 1.0879125816572923 0.00032825023596867546\n",
      "801 1.262531905740616 0.0003280241122224816\n",
      "901 1.050877535046311 0.0003277984551471088\n",
      "1001 1.135452825037646 0.0003275732631395822\n",
      "1101 1.0099836179433623 0.0003273485346046242\n",
      "1201 1.1641883124248125 0.0003271242679546084\n",
      "1301 1.0249766572378576 0.00032690046160951133\n",
      "1401 1.216345368164184 0.0003266771139968662\n",
      "1501 1.4009095890432945 0.00032645422355171653\n",
      "1601 0.9214687866624445 0.00032623178871657\n",
      "1701 1.050587208737852 0.0003260098079413526\n",
      "1801 1.0106142781150993 0.0003257882796833635\n",
      "1901 0.9388233295176178 0.00032556720240723\n",
      "2001 1.1458081254386343 0.0003253465745848626\n",
      "2101 1.0818304931126477 0.00032512639469541087\n",
      "2201 0.8635702040046453 0.0003249066612252194\n",
      "2301 1.0180434776411857 0.00032468737266778394\n",
      "2401 1.0467977939988486 0.0003244685275237081\n",
      "2501 1.083000476603047 0.0003242501243006605\n",
      "2601 1.1069668279960752 0.00032403216151333166\n",
      "2701 1.086160118225962 0.00032381463768339173\n",
      "2801 1.0924749624973629 0.0003235975513394485\n",
      "2901 1.009744831302669 0.00032338090101700554\n",
      "3001 0.9085385013604537 0.0003231646852584205\n",
      "3101 1.1706041378201917 0.00032294890261286426\n",
      "3201 1.032271361502353 0.00032273355163627964\n",
      "3301 1.2584509218577296 0.00032251863089134133\n",
      "3401 1.2436874122358859 0.000322304138947415\n",
      "3501 1.0451832013077365 0.0003220900743805179\n",
      "3601 1.0900762653473066 0.00032187643577327854\n",
      "3701 1.1192542002827395 0.00032166322171489793\n",
      "3801 1.00253647408681 0.0003214504308011099\n",
      "3901 1.2160937447333708 0.0003212380616341424\n",
      "4001 1.0416435159859248 0.0003210261128226793\n",
      "4101 1.3598752447869629 0.00032081458298182156\n",
      "4201 1.0555532689650136 0.0003206034707330495\n",
      "4301 1.1295962483854964 0.00032039277470418526\n",
      "4401 0.9410244208120275 0.0003201824935293548\n",
      "4501 0.8939700378105044 0.00031997262584895135\n",
      "4601 0.908640876179561 0.000319763170309598\n",
      "4701 1.128680162204546 0.0003195541255641112\n",
      "4801 1.0497526655672118 0.00031934549027146444\n",
      "4901 1.0289937005145475 0.0003191372630967521\n",
      "5001 0.9918764412868768 0.0003189294427111535\n",
      "5101 1.2255524442298338 0.0003187220277918973\n",
      "5201 1.3292681298672733 0.0003185150170222263\n",
      "5301 0.878005885053426 0.00031830840909136197\n",
      "5401 1.0740752452165907 0.00031810220269447\n",
      "5501 1.0994656120092259 0.00031789639653262544\n",
      "5601 1.159670107124839 0.0003176909893127784\n",
      "5701 0.8859041188843548 0.0003174859797477199\n",
      "5801 1.084522244927939 0.00031728136655604814\n",
      "5901 1.4824702723776682 0.0003170771484621346\n",
      "6001 1.230977819112013 0.0003168733241960908\n",
      "6101 1.0119965468620649 0.00031666989249373517\n",
      "6201 1.1002646164814678 0.00031646685209656003\n",
      "6301 1.1440792203939054 0.00031626420175169897\n",
      "1 1.0551144047021808 0.0003160882122689669\n",
      "101 1.0408390048833098 0.00031588628797931984\n",
      "201 1.0153360446565785 0.0003156847501772966\n",
      "301 1.01748421555385 0.0003154835976315582\n",
      "401 1.1267781729111448 0.0003152828291162512\n",
      "501 1.0327468327741371 0.0003150824434109757\n",
      "601 0.9960314880299848 0.0003148824393007546\n",
      "701 1.0631007702104398 0.00031468281557600267\n",
      "801 1.0372768385277595 0.00031448357103249544\n",
      "901 1.031023440795252 0.0003142847044713392\n",
      "1001 0.9323838343843818 0.0003140862146989404\n",
      "1101 0.850510573014617 0.0003138881005269756\n",
      "1201 1.0943628003296908 0.0003136903607723615\n",
      "1301 1.0844742289336864 0.00031349299425722566\n",
      "1401 1.4024500491796061 0.00031329599980887637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501 1.1463393379817717 0.00031309937625977405\n",
      "1601 1.0650637014914537 0.0003129031224475018\n",
      "1701 1.1410465109511279 0.00031270723721473664\n",
      "1801 1.0148204645956866 0.0003125117194092209\n",
      "1901 0.9743651752360165 0.0003123165678837336\n",
      "2001 0.990701739974611 0.00031212178149606226\n",
      "2101 1.1128740338463103 0.00031192735910897496\n",
      "2201 1.5508626039809315 0.0003117332995901923\n",
      "2301 1.01486110695987 0.00031153960181235955\n",
      "2401 0.9611194784665713 0.0003113462646530196\n",
      "2501 1.0847897573257796 0.0003111532869945851\n",
      "2601 1.2803321699175285 0.00031096066772431187\n",
      "2701 1.0769891024538083 0.0003107684057342714\n",
      "2801 1.0039243546780199 0.00031057649992132457\n",
      "2901 1.0824949400266632 0.00031038494918709473\n",
      "3001 0.9644128995714709 0.00031019375243794144\n",
      "3101 0.9868561172188492 0.00031000290858493437\n",
      "3201 1.0903575613629073 0.00030981241654382685\n",
      "3301 1.2633273452374851 0.0003096222752350304\n",
      "3401 1.0088038056419464 0.000309432483583589\n",
      "3501 1.1047235757578164 0.0003092430405191533\n",
      "3601 1.163446888080216 0.00030905394497595545\n",
      "3701 1.018205283649877 0.00030886519589278384\n",
      "3801 1.0408570388099179 0.00030867679221295824\n",
      "3901 1.0657447287230752 0.00030848873288430483\n",
      "4001 0.930832964291767 0.0003083010168591314\n",
      "4101 1.4587874389944773 0.00030811364309420327\n",
      "4201 1.1227497690124437 0.0003079266105507184\n",
      "4301 1.1970081577601377 0.0003077399181942835\n",
      "4401 1.1083186157047749 0.00030755356499488986\n",
      "4501 1.0473160178516991 0.00030736754992688985\n",
      "4601 1.0928417469840497 0.0003071818719689727\n",
      "4701 1.0073067757184617 0.00030699653010414117\n",
      "4801 1.1523846584532293 0.00030681152331968824\n",
      "4901 1.1988015054084826 0.0003066268506071739\n",
      "5001 1.036811558995396 0.00030644251096240176\n",
      "5101 1.0675939484208357 0.0003062585033853964\n",
      "5201 1.0752534797684348 0.00030607482688038056\n",
      "5301 1.1183025861246279 0.0003058914804557523\n",
      "5401 1.0365500289481133 0.0003057084631240629\n",
      "5501 1.129350705537945 0.00030552577390199393\n",
      "5601 1.06671357084997 0.0003053434118103358\n",
      "5701 1.0859052878222428 0.0003051613758739652\n",
      "5801 1.0723684425465763 0.00030497966512182316\n",
      "5901 1.0603833084605867 0.0003047982785868937\n",
      "6001 1.0581669194652932 0.0003046172153061818\n",
      "6101 1.2675022858026068 0.0003044364743206923\n",
      "6201 1.0536423055746127 0.0003042560546754083\n",
      "6301 1.2465427681345318 0.00030407595541926996\n",
      "1 0.8441335130482912 0.00030391413923858634\n",
      "101 0.9350836968515068 0.00030373464611573535\n",
      "201 1.0421846130630001 0.0003035554706461405\n",
      "301 1.008769184758421 0.0003033766118939749\n",
      "401 0.9787611065548845 0.000303198068927267\n",
      "501 1.0469113910803571 0.00030301984081788013\n",
      "601 1.0306272330635693 0.00030284192664149214\n",
      "701 0.8391264111269265 0.00030266432547757535\n",
      "801 0.9852646276758605 0.00030248703640937665\n",
      "901 0.9640902730752714 0.00030231005852389745\n",
      "1001 1.1280414578068303 0.00030213339091187405\n",
      "1101 0.9939612101297826 0.0003019570326677579\n",
      "1201 1.0867023059399799 0.00030178098288969626\n",
      "1301 1.0411206257122103 0.00030160524067951265\n",
      "1401 1.0711584523378406 0.0003014298051426879\n",
      "1501 1.0796883448783774 0.0003012546753883405\n",
      "1601 1.027666941517964 0.00030107985052920836\n",
      "1701 1.082986782770604 0.00030090532968162913\n",
      "1801 1.1074479344606516 0.0003007311119655219\n",
      "1901 1.1305997944582487 0.0003005571965043686\n",
      "2001 1.345339710366943 0.0003003835824251953\n",
      "2101 1.001590578132891 0.00030021026885855383\n",
      "2201 1.0054450589232147 0.0003000372549385033\n",
      "2301 1.0041432639409322 0.00029986453980259265\n",
      "2401 1.0640304164699046 0.00029969212259184163\n",
      "2501 1.1201181028736755 0.0002995200024507235\n",
      "2601 0.8765224074013531 0.00029934817852714696\n",
      "2701 1.0152945614827331 0.0002991766499724386\n",
      "2801 1.2956223709957158 0.0002990054159413251\n",
      "2901 1.097986907014274 0.0002988344755919157\n",
      "3001 1.2291080165232415 0.00029866382808568526\n",
      "3101 1.1140619127836544 0.0002984934725874564\n",
      "3201 1.0722678579004423 0.0002983234082653825\n",
      "3301 1.03946516571159 0.0002981536342909311\n",
      "3401 0.9876702070032479 0.0002979841498388662\n",
      "3501 0.8540887358831242 0.00029781495408723205\n",
      "3601 0.9894529741141014 0.00029764604621733594\n",
      "3701 1.0710785342380404 0.000297477425413732\n",
      "3801 1.2710673977526312 0.00029730909086420423\n",
      "3901 1.0929949116252828 0.0002971410417597504\n",
      "4001 1.1222996068827342 0.0002969732772945655\n",
      "4101 1.164539644116303 0.00029680579666602566\n",
      "4201 1.033734397671651 0.000296638599074672\n",
      "4301 1.093015514779836 0.0002964716837241944\n",
      "4401 1.0481613585725427 0.0002963050498214161\n",
      "4501 1.0937337041832507 0.00029613869657627706\n",
      "4601 1.1815662420112858 0.000295972623201819\n",
      "4701 1.1428916620570817 0.0002958068289141693\n",
      "4801 1.1009264337189961 0.0002956413129325257\n",
      "4901 0.8777621657354757 0.00029547607447914055\n",
      "5001 1.0156730558082927 0.00029531111277930595\n",
      "5101 0.9942513670539483 0.00029514642706133804\n",
      "5201 1.1302866424011881 0.00029498201655656206\n",
      "5301 1.0087051462905947 0.0002948178804992971\n",
      "5401 1.0660143050336046 0.0002946540181268415\n",
      "5501 1.0107977577135898 0.00029449042867945755\n",
      "5601 1.0777363086963305 0.0002943271114003569\n",
      "5701 0.856828257907182 0.00029416406553568584\n",
      "5801 1.1287360956775956 0.0002940012903345107\n",
      "5901 1.366358119645156 0.00029383878504880313\n",
      "6001 1.0964845723065082 0.00029367654893342604\n",
      "6101 1.34646458978159 0.00029351458124611887\n",
      "6201 1.2197050878312439 0.0002933528812474836\n",
      "6301 1.0830936049751472 0.0002931914482009704\n",
      "1 1.2537849597129025 0.00029304477550482497\n",
      "101 0.9655292083625682 0.00029288385030021\n",
      "201 1.1372923650196753 0.0002927231899204302\n",
      "301 0.9451354363700375 0.0002925627936399378\n",
      "401 0.9661671929707154 0.00029240266073596516\n",
      "501 1.0162687979172915 0.0002922427904885108\n",
      "601 0.9551542007829994 0.0002920831821803257\n",
      "701 0.9841917234880384 0.0002919238350969\n",
      "801 1.061543255826109 0.00029176474852644945\n",
      "901 0.985908080736408 0.0002916059217599022\n",
      "1001 1.0337736615701942 0.0002914473540908853\n",
      "1101 0.9899544979416532 0.0002912890448157118\n",
      "1201 1.1052642236463726 0.00029113099323336726\n",
      "1301 0.9609193275682628 0.00029097319864549706\n",
      "1401 1.0143777604680508 0.0002908156603563932\n",
      "1501 1.0131031578639522 0.0002906583776729816\n",
      "1601 1.0399196342332289 0.00029050134990480915\n",
      "1701 1.3567781529854983 0.00029034457636403104\n",
      "1801 1.1145936762022757 0.0002901880563653981\n",
      "1901 1.0984270876506343 0.0002900317892262443\n",
      "2001 1.0226630433771788 0.00028987577426647405\n",
      "2101 1.1856945900362916 0.0002897200108085499\n",
      "2201 1.125245438015554 0.00028956449817748025\n",
      "2301 1.126162831991678 0.00028940923570080693\n",
      "2401 0.971063018507266 0.00028925422270859307\n",
      "2501 0.8773902256507427 0.00028909945853341086\n",
      "2601 0.8763325407635421 0.0002889449425103295\n",
      "2701 1.1415061227562546 0.0002887906739769035\n",
      "2801 1.052460735765635 0.0002886366522731603\n",
      "2901 1.5205009760629764 0.00028848287674158846\n",
      "3001 0.9414957111439435 0.0002883293467271265\n",
      "3101 0.9435001520323567 0.0002881760615771502\n",
      "3201 1.1403545759221743 0.0002880230206414618\n",
      "3301 1.053262686386006 0.00028787022327227786\n",
      "3401 1.0832857484929264 0.000287717668824218\n",
      "3501 1.1019753235159442 0.00028756535665429354\n",
      "3601 1.055358653771691 0.0002874132861218958\n",
      "3701 1.0278627741499804 0.00028726145658878504\n",
      "3801 1.0083879251906183 0.0002871098674190792\n",
      "3901 1.0555589499126654 0.0002869585179792425\n",
      "4001 1.0509156602493022 0.00028680740763807453\n",
      "4101 1.0385481148259714 0.0002866565357666993\n",
      "4201 1.0417402729653986 0.0002865059017385537\n",
      "4301 1.082753369351849 0.0002863555049293774\n",
      "4401 1.0459589868987678 0.0002862053447172013\n",
      "4501 1.0592919351911405 0.00028605542048233684\n",
      "4601 1.1034397517796606 0.0002859057316073656\n",
      "4701 1.1311876591207692 0.00028575627747712837\n",
      "4801 1.070465801298269 0.00028560705747871445\n",
      "4901 1.199700104945805 0.00028545807100145134\n",
      "5001 1.2379299406893551 0.00028530931743689397\n",
      "5101 1.1045849512156565 0.00028516079617881457\n",
      "5201 0.9958119990806154 0.000285012506623192\n",
      "5301 1.620139996672151 0.00028486444816820157\n",
      "5401 1.0613090786646353 0.0002847166202142048\n",
      "5501 1.175794189737644 0.0002845690221637393\n",
      "5601 1.1241089710965753 0.00028442165342150834\n",
      "5701 1.160597581154434 0.000284274513394371\n",
      "5801 1.0310369414401066 0.0002841276014913322\n",
      "5901 0.9960121748881647 0.0002839809171235324\n",
      "6001 0.9698299318188219 0.00028383445970423817\n",
      "6101 1.1415085992775857 0.0002836882286488319\n",
      "6201 1.0641657677479088 0.0002835422233748022\n",
      "6301 1.0828722650112468 0.00028339644330173413\n"
     ]
    }
   ],
   "source": [
    "#weight = torch.ones(len(TGT.vocab))\n",
    "#weight[pad_idx] = 0\n",
    "#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35JC6i9QUgSB"
   },
   "outputs": [],
   "source": [
    "1 10.825187489390373 6.987712429686844e-07\n",
    "101 9.447168171405792 3.56373333914029e-05\n",
    "201 7.142856806516647 7.057589553983712e-05\n",
    "301 6.237934365868568 0.00010551445768827134\n",
    "401 5.762486848048866 0.00014045301983670557\n",
    "501 5.415792358107865 0.00017539158198513977\n",
    "601 5.081815680023283 0.000210330144133574\n",
    "701 4.788327748770826 0.00024526870628200823\n",
    "801 4.381739928154275 0.0002802072684304424\n",
    "901 4.55433791608084 0.00031514583057887664\n",
    "1001 4.911875109748507 0.0003500843927273108\n",
    "1101 4.0579032292589545 0.0003850229548757451\n",
    "1201 4.2276234351193125 0.0004199615170241793\n",
    "1301 3.932735869428143 0.00045490007917261356\n",
    "1401 3.8179439397063106 0.0004898386413210477\n",
    "1501 3.3608515430241823 0.000524777203469482\n",
    "1601 3.832796103321016 0.0005597157656179162\n",
    "1701 2.907085266895592 0.0005946543277663504\n",
    "1801 3.5280659823838505 0.0006295928899147847\n",
    "1901 2.895841649500653 0.0006645314520632189\n",
    "2001 3.273784235585481 0.000699470014211653\n",
    "2101 3.181488689899197 0.0007344085763600873\n",
    "2201 3.4151616653980454 0.0007693471385085215\n",
    "2301 3.4343731447588652 0.0008042857006569557\n",
    "2401 3.0505455391539726 0.0008392242628053899\n",
    "2501 2.8089329147478566 0.0008741628249538242\n",
    "2601 2.7827929875456903 0.0009091013871022583\n",
    "2701 2.4428516102489084 0.0009440399492506926\n",
    "2801 2.4015486147254705 0.0009789785113991267\n",
    "2901 2.3568112018401735 0.001013917073547561\n",
    "3001 2.6349758653668687 0.0010488556356959952\n",
    "3101 2.5981983028614195 0.0010837941978444295\n",
    "3201 2.666826274838968 0.0011187327599928637\n",
    "3301 3.0092043554177508 0.0011536713221412978\n",
    "3401 2.4580375660589198 0.0011886098842897321\n",
    "3501 2.586465588421561 0.0012235484464381662\n",
    "3601 2.5663993963389657 0.0012584870085866006\n",
    "3701 2.9430236657499336 0.0012934255707350347\n",
    "3801 2.464644919440616 0.001328364132883469\n",
    "3901 2.7124062888276512 0.0013633026950319032\n",
    "4001 2.646443709731102 0.0013971932312809247\n",
    "4101 2.7294750874862075 0.001380057517579748\n",
    "4201 2.1295202329056337 0.0013635372009002666\n",
    "4301 2.596563663915731 0.001347596306985731\n",
    "4401 2.1265982036820787 0.0013322017384983986\n",
    "4501 2.3880532500334084 0.0013173229858148\n",
    "4601 2.6129120760888327 0.0013029318725783852\n",
    "4701 2.2873719420749694 0.001289002331178292\n",
    "4801 2.4949760700110346 0.0012755102040816328\n",
    "4901 2.496607314562425 0.001262433067573089\n",
    "5001 2.1889712483389303 0.0012497500749750088\n",
    "5101 1.8677761815488338 0.0012374418168536253\n",
    "5201 2.2992054556962103 0.0012254901960784316\n",
    "5301 2.664361578106707 0.0012138783159049418\n",
    "5401 2.705850490485318 0.0012025903795063202\n",
    "5501 2.581445264921058 0.0011916115995949978\n",
    "5601 2.2480602325085783 0.0011809281169581616\n",
    "5701 1.9289666265249252 0.0011705269268863989\n",
    "5801 2.4863578918157145 0.0011603958126073107\n",
    "5901 2.632946971571073 0.0011505232849492607\n",
    "6001 2.496141305891797 0.0011408985275576757\n",
    "6101 2.6422974687084206 0.0011315113470699342\n",
    "6201 2.448802186456305 0.0011223521277270118"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aVdpQ_KwUgQv",
    "Lz6n3REAUgRH",
    "52xwbbb4UgRK",
    "MPp__T_uUgRK",
    "8tkzxQYKUgRQ",
    "XoyfFgLoUgRW"
   ],
   "name": "Copy of The Annotated \"Attention is All You Need\".ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
